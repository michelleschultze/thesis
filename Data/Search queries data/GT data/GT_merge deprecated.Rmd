Import and merge GT data
Tuesday, February 25 2025

This file will import the manually downloaded GT data, merge into a monthly series, and export (for later additions of covariates).

- - - - - - - - - - - - - - - - - - - -

```{r}
library(forecast)
library(tseries)
library(readr)
library(stringr)
library(lubridate)
library(dplyr)
library(zoo)
library(readxl)
library(tidyverse)
library(glmnet)
library(sp)
set.seed(42)
```


```{r}
# Define the directory containing the GT data
data_dir <- "/Users/michelle/Documents/thesis/Search queries data/GT data/raw keyword data"

# List all CSV files in the directory
files <- list.files(data_dir, pattern = "\\.csv$", full.names = TRUE)

# Initialize an empty list to store the dataframes
data_list <- list()

# Loop through each file, read the data, and process it
for (file in files) {
  # Extract the keyword (filename without extension) for the column name
  keyword <- basename(file) %>% str_remove(".csv")
  
  # Read the CSV file
  temp_df <- read_csv(file, skip = 1)  # Assuming the first row is not part of the actual data
  
  # Rename the columns: 'Month' and 'Frequency'
  colnames(temp_df) <- c("Month", keyword)
  
  # Clean up the 'Month' column to ensure it's in a proper date format
  temp_df$Month <- ym(temp_df$Month)
  
  # Group by 'Month' and summarize by taking the sum of frequencies (or use mean if needed)
  temp_df <- temp_df %>%
    group_by(Month) %>%
    summarise(across(starts_with(keyword), mean, na.rm = TRUE))
  
  # Add this dataframe to the list
  data_list[[keyword]] <- temp_df
}

# Merge all the dataframes in the list by the 'Month' column
merged_df <- Reduce(function(x, y) full_join(x, y, by = "Month"), data_list)

# View the merged dataframe
head(merged_df)

merged_df_copy <- merged_df
```

## Monthly set

```{r}
# Loop through every column except 'Month'
merged_df <- merged_df %>%
  mutate(
    # Create log_* columns for each numeric variable
    across(.cols = -Month, .fns = ~ if_else(. > -1, log1p(.), NA_real_), .names = "log_{.col}")
  )
```

```{r}
# Generate ratio_* and diff_* variables
merged_df <- merged_df %>%
  mutate(
    work_ratio = jumush / rabota,
    #yunistream_ratio = yunistream_kyrgyzstan / yunistream, (this is 0/0 for many)
    findwork_ratio = jumush_izdeim / poisk_raboti,
    salary_ratio = ailyk / zarplata,
    #zolotaya_korona_ratio = zolotaya_korona_kyrgyzstan / zolotaya_korona,  (this is 0/0 for many)
    
    # Generate diff_* variables with reversed order (Russia - Kyrgyzstan)
    work_diff = rabota - jumush,
    yunistream_diff = yunistream - yunistream_kyrgyzstan,
    findwork_diff = poisk_raboti - jumush_izdeim,
    salary_diff = zarplata - ailyk,
    zolotaya_korona_diff = zolotaya_korona - zolotaya_korona_kyrgyzstan
  )
```

```{r}
# Loop through every column except 'Month'
merged_df <- merged_df %>%
  mutate(
    
    # Create lag_* columns for each numeric variable (including logs)
    across(.cols = -Month, .fns = lag, .names = "lag_{.col}")
  )
```

```{r}
merged_df <- merged_df %>%
  filter(Month >= ymd("2011-01-01"),
         Month <= ymd("2025-01-01")) 
```

```{r}
# Save the merged dataframe to a CSV file (optional)
write_csv(merged_df, "/Users/michelle/Documents/thesis/Search queries data/GT data/merged_gt_data_monthly.csv")
```


## AR + Quarterly set

Make quarterly
```{r}
library(dplyr)
library(lubridate)

# Convert monthly data to quarterly by summing values within each quarter
quarterly_df <- merged_df_copy %>%
  mutate(Quarter = floor_date(Month, "quarter")) %>%  # Assign each month to its quarter
  group_by(Quarter) %>%
  summarise(across(where(is.numeric), sum, na.rm = TRUE)) %>%  # Sum all numeric columns
  ungroup()
```

```{r}
# Loop through every column except 'Quarter'
quarterly_df <- quarterly_df %>%
  mutate(
    # Create log_* columns for each numeric variable
    across(.cols = -Quarter, .fns = ~ if_else(. > -1, log1p(.), NA_real_), .names = "log_{.col}")
  )
```

```{r}
# Generate ratio_* and diff_* variables
quarterly_df <- quarterly_df %>%
  mutate(
    work_ratio = jumush / rabota,
    #yunistream_ratio = yunistream_kyrgyzstan / yunistream, (this is 0/0 for many)
    findwork_ratio = jumush_izdeim / poisk_raboti,
    salary_ratio = ailyk / zarplata,
    #zolotaya_korona_ratio = zolotaya_korona_kyrgyzstan / zolotaya_korona,  (this is 0/0 for many)
    
    # Generate diff_* variables with reversed order (Russia - Kyrgyzstan)
    work_diff = rabota - jumush,
    yunistream_diff = yunistream - yunistream_kyrgyzstan,
    findwork_diff = poisk_raboti - jumush_izdeim,
    salary_diff = zarplata - ailyk,
    zolotaya_korona_diff = zolotaya_korona - zolotaya_korona_kyrgyzstan
  )
```

```{r}
# Loop through every column except 'Quarter'
quarterly_df <- quarterly_df %>%
  mutate(
    
    # Create lag_* columns for each numeric variable (including logs)
    across(.cols = -Quarter, .fns = lag, .names = "lag_{.col}")
  )
```

```{r}
quarterly_df <- quarterly_df %>%
  filter(Quarter >= ymd("2010-01-01"),
         Quarter < ymd("2025-01-01")) 
```

Prep to merge the target variable into the dataframe
```{r}
inflows_quarterly <- read_excel("~/Documents/thesis/Macrodata/Cleaned macrodata (intermediate)/QUARTERLY_MIGRANT_FLOWS_CLEANED.xls")

# Pivot the data to long format
inflows_quarterly <- inflows_quarterly %>%
  pivot_longer(
    cols = -`Type of Visitation`,  # Keep the "Type of Visitation" column as is
    names_to = "Quarter",          # New column for quarter labels
    values_to = "Value"            # New column for values
  ) %>%
  arrange(`Type of Visitation`, Quarter)

# Pivot to wide format to make each visitation type a separate column
inflows_quarterly <- inflows_quarterly %>%
  pivot_wider(
    names_from = `Type of Visitation`,  # Use visitation type as column names
    values_from = Value                 # Fill with corresponding values
  ) %>%
  arrange(Quarter)

# Convert Quarter to a date-like format (yearqtr from zoo)
inflows_quarterly <- inflows_quarterly %>%
  mutate(Quarter = as.yearqtr(Quarter, format = "%Y Q%q"))

#Take only the total variable
inflows_quarterly <- inflows_quarterly %>%
  mutate(Kyrgyz_visitors = inflows_quarterly$'total visitors, all types') %>%
  select(Quarter, Kyrgyz_visitors)
```


Drop the levels to prevent heterosked. 
```{r}
cols_to_drop <- c(
  "ailyk", "aviabilet_bishkek", "bilet_bishkek", "jumush_izdeim", "jumush", 
  "perevodi_deneg", "poisk_raboti", "rabota", "vakansii", "yunistream", 
  "zarplata", "zolotaya_korona",
  "lag_ailyk", "lag_aviabilet_bishkek", "lag_bilet_bishkek", "lag_jumush_izdeim", "lag_jumush", 
  "lag_perevodi_deneg", "lag_poisk_raboti", "lag_rabota", "lag_vakansii", "lag_yunistream", 
  "lag_zarplata", "lag_zolotaya_korona"
)

quarterly_df <- quarterly_df[, !(names(quarterly_df) %in% cols_to_drop)]
```


Extending the time series

1. Prepare the data (Ensure it's in quarterly format).
2. Create seasonal dummies (if seasonality is present).
3. Add a time trend.
4. Use stepwise AIC selection to choose the best covariates.
5. Fit an ARIMA model incorporating the selected covariates.
6. Generate forecasts to extend the time series.



```{r}
# Ensure data is quarterly
quarterly_df <- quarterly_df %>%
  mutate(Quarter = as.yearqtr(Quarter)) %>%
  arrange(Quarter)

quarterly_df_copy <- quarterly_df
```

```{r}
# Merge datasets on "Quarter"
quarterly_df <- inflows_quarterly %>%
  left_join(quarterly_df, by = "Quarter")
```

Shorten the covariate series for estimation of the model
```{r}
quarterly_df_2010_16 <- quarterly_df %>%
  filter(!is.na(Kyrgyz_visitors)) %>%
  filter(complete.cases(.))  # Remove rows with NA in any of the predictors
```


```{r}
# Convert time series to ts object
y_full <- quarterly_df_2010_16$Kyrgyz_visitors
```


Seasonal dummies and AR(4)
```{r}
# Create AR(4) features for Kyrgyz_visitors
quarterly_df_2010_16 <- quarterly_df %>%
  mutate(
    # Create lagged features for AR(4)
    AR1 = lag(Kyrgyz_visitors, 1),
    AR2 = lag(Kyrgyz_visitors, 2),
    AR3 = lag(Kyrgyz_visitors, 3),
    AR4 = lag(Kyrgyz_visitors, 4)
  ) %>%
  # Drop rows with NA values caused by the lags (the first 4 rows will have NAs for the AR variables)
  filter(!is.na(AR1) & !is.na(AR2) & !is.na(AR3) & !is.na(AR4))

# Create seasonal dummies based on the 'Quarter' column
seasonal_dummies <- model.matrix(~ as.factor(quarter(Quarter)) - 1, data = quarterly_df_2010_16)

# Convert the resulting matrix to a data frame
seasonal_dummies_df <- as.data.frame(seasonal_dummies)

# Set appropriate column names
colnames(seasonal_dummies_df) <- paste0("Q", 1:ncol(seasonal_dummies_df))

head(quarterly_df_2010_16)
```

```{r}
# Convert time series to ts object
y <- quarterly_df_2010_16$Kyrgyz_visitors
```

Covar names
```{r}
# Select potential covariates
covariate_names <- setdiff(names(quarterly_df_2010_16), c("Quarter", "Kyrgyz_visitors"))
```

Scale features according to limited frame and save the scaler for later
```{r}
# Scale the predictor variables (excluding the target variable 'Kyrgyz_visitors')
quarterly_df_2010_16 <- scale(quarterly_df_2010_16[, covariate_names])

# Step 2: Scale the predictor variables (excluding the target variable 'Kyrgyz_visitors')
# Save the scaling parameters (mean and standard deviation) for later use
scaler <- quarterly_df_2010_16
```

Some cause NAs because there's no variation! drop those columns, and save the names for later.
```{r}
# Check for NAs in the dataset
na_count <- colSums(is.na(quarterly_df_2010_16))

# Identify columns with NAs and no variation
cols_to_drop <- names(na_count[na_count > 0])

# Save column names with NAs for later
na_columns <- cols_to_drop

# Optionally, print the names of the columns removed
print(na_columns)

quarterly_df_2010_16 <- as.data.frame(quarterly_df_2010_16)

# Remove the identified columns
quarterly_df_2010_16 <- quarterly_df_2010_16[, !(names(quarterly_df_2010_16) %in% cols_to_drop)]
```

1. Take the top correlated features with the y (including AR) because otherwise we will have p>n, and we need n>p to do good analysis
```{r}
# Example of using the correlation test for continuous features
cor_values <- apply(quarterly_df_2010_16, 2, function(x) cor(x, y))  # Assuming 'y' is the target variable

# Sort the correlations in decreasing order (highest correlation first)
sorted_cor <- sort(cor_values, decreasing = TRUE)

# Select the top 27 features based on least correlation with the target
top_features <- names(sorted_cor)[1:27]

# Subset the data to include only the selected features
selected_data <- quarterly_df_2010_16[, top_features]

# Check the selected features
print(top_features)

quarterly_df_2010_16 <- quarterly_df_2010_16 %>%
  select(top_features)
```

```{r}
# Select potential covariates
covariate_names <- setdiff(names(quarterly_df_2010_16), c("Quarter", "Kyrgyz_visitors"))
```

2. Run PCA to reduce dimensionality and propensity to overfit. Add back seasonal dummies afterwards.
```{r}
# Load necessary package
library(stats)

# Perform PCA on the scaled predictors
pca_result <- prcomp(quarterly_df_2010_16, center = TRUE, scale. = TRUE)

# Determine how many principal components to keep (e.g., 95% variance explained)
explained_variance <- summary(pca_result)$importance[2,]  # Proportion of variance
cumulative_variance <- cumsum(explained_variance)

# Find the number of components explaining at least 95% variance
num_components <- which(cumulative_variance >= 0.95)[1]

# Keep only the first `num_components` principal components
pca_df <- as.data.frame(pca_result$x[, 1:num_components])

# Add seasonal dummies back (they were not included in PCA)
pca_df <- cbind(pca_df, seasonal_dummies_df)

# Ensure PCA dataframe has proper column names
colnames(pca_df) <- c(paste0("PC", 1:num_components), colnames(seasonal_dummies_df))
```

3. Fit elastic net model
```{r}
library(glmnet)

# Convert response variable to matrix format for LASSO
y <- as.matrix(y)

# Convert PCA-transformed predictors to matrix format
X <- as.matrix(pca_df)

# Run LASSO regression
lasso_model <- cv.glmnet(X, y, alpha = 1, standardize = FALSE)

# Get the best lambda value
best_lambda <- lasso_model$lambda.min

# Fit final model using best lambda
final_lasso <- glmnet(X, y, alpha = 1, lambda = best_lambda, standardize = FALSE)

# Extract coefficients
coef(final_lasso)
```

~ ~ ~

Prep to predict
```{r}
quarterly_df <- quarterly_df_copy

# Create seasonal dummies based on the 'Quarter' column
seasonal_dummies <- model.matrix(~ as.factor(quarter(Quarter)) - 1, data = quarterly_df)

# Convert the resulting matrix to a data frame
seasonal_dummies_df <- as.data.frame(seasonal_dummies)

# Set appropriate column names
colnames(seasonal_dummies_df) <- paste0("Q", 1:ncol(seasonal_dummies_df))

# Add fake AR1, AR2, AR3, AR4 columns to the quarterly_df dataframe with NA values so the scaler doesn't bug out
quarterly_df <- quarterly_df %>%
  mutate(
    AR1 = NA,
    AR2 = NA,
    AR3 = NA,
    AR4 = NA
  ) 

quarterly_df <- quarterly_df %>%
  select(-Quarter)

covariate_names <- setdiff(names(quarterly_df), c("Kyrgyz_visitors"))

setdiff(names(quarterly_df), covariate_names)
setdiff(covariate_names, names(quarterly_df))

# Bind the seasonal dummies to the original data frame (after scaling)
quarterly_df <- cbind(quarterly_df, seasonal_dummies_df)

# Check the scaled data
head(quarterly_df)

quarterly_df <- quarterly_df %>%
  select(top_features)

covariate_names <- setdiff(names(quarterly_df), c("Quarter", "Kyrgyz_visitors"))
```

AR scaler
```{r}
# Extract center and scale attributes
ARscaler_params <- attr(scaler, "scaled:center")[c("AR1", "AR3", "AR4")]
ARscaler_sd <- attr(scaler, "scaled:scale")[c("AR1", "AR3", "AR4")]
```



```{r}
# Initialize forecast horizon
forecast_horizon <- 32  # Adjust as needed

# Store the predictions
predictions <- numeric(forecast_horizon)

# Extract the last available AR values from the dataset
last_AR_values <- as.numeric(tail(quarterly_df_2010_16[, c("AR1", "AR3", "AR4")], 1))

num_components <- 8 #selected earlier by PCA

# Iterative forecasting loop
for (t in 1:forecast_horizon) {
  print(paste("Starting next iteration...", t))
  
  # Create a new row for forecasting (using the last available row in the dataset)
  new_X <- tail(quarterly_df_2010_16, 1)  # Start with the last row of the dataset
  new_X[, c("AR1", "AR3", "AR4")] <- last_AR_values  # Replace AR values
  
  # Scale the AR values using the 'ARscaler' (mean and standard deviation of 'y')
  new_X[, c("AR1", "AR3", "AR4")] <- scale(new_X[, c("AR1", "AR3", "AR4")], 
                                                   center = ARscaler_params, 
                                                   scale = ARscaler_sd)
  
  # Apply PCA transformation on the scaled data
  new_X_pca <- predict(pca_result, new_X)
  
  pc_values <- new_X_pca[, 1:num_components, drop = TRUE] 

  # Convert named vector to a one-row dataframe
  new_X_pca_9 <- as.data.frame(t(pc_values))
  
  # Dynamically update the seasonal dummy based on the forecasted quarter
  forecasted_quarter <- (as.numeric(quarter(Sys.Date())) + t-1) %% 4
  forecasted_quarter[forecasted_quarter == 0] <- 4  # Convert 0 to Q4
  
  # Add quarter dummies
  new_X_pca_9 <- new_X_pca_9 %>%
    mutate(Q1 = ifelse(forecasted_quarter == 1, 1, 0),
           Q2 = ifelse(forecasted_quarter == 2, 1, 0),
           Q3 = ifelse(forecasted_quarter == 3, 1, 0),
           Q4 = ifelse(forecasted_quarter == 4, 1, 0))
  
  # Predict the next step using the trained Elastic Net model
  next_prediction <- predict(final_lasso, new_X_pca_9, s = best_lambda)
  
  # Store the prediction
  predictions[t] <- next_prediction
  
  # Update AR values: Shift and add the new prediction
  last_AR_values <- c(next_prediction, last_AR_values[1:3])
}

```

```{r}
# Step 4: Prepare the results dataframe
combined_actual <- c(inflows_quarterly$Kyrgyz_visitors, rep(NA, length(predictions)))
combined_predictions <- c(rep(NA, length(inflows_quarterly$Kyrgyz_visitors)), predictions)

# Combine y and predictions into a single dataframe for plotting
results_df <- data.frame(
  Quarter = quarterly_df_copy$Quarter,
  Actual = combined_actual,
  Predictions = combined_predictions
)
```

Plot!
```{r}
# Plotting with ggplot
ggplot(results_df, aes(x = Quarter)) +
  geom_line(aes(y = Actual, color = "Actual"), size = 1) +
  geom_line(aes(y = Predictions, color = "Predictions"), size = 1, linetype = "dashed") +
  labs(title = "Actual vs Predicted Kyrgyz Visitors",
       x = "Quarter",
       y = "Kyrgyz Visitors") +
  scale_color_manual(name = "Legend", values = c("Actual" = "blue", "Predictions" = "red")) +
  theme_minimal() +
  geom_vline(xintercept = as.numeric(as.yearqtr("2022 Q1")), linetype = "dashed", color = "black") +  # Vertical line for War starts
  annotate("text", x = as.yearqtr("2022 Q1"), y = max(results_df$Actual, na.rm = TRUE) * 0.9, 
           label = "War begins", angle = 90, hjust = 2, vjust = 0.5, color = "black")
  
ggsave("/Users/michelle/Documents/thesis/plot_extended_time_series_visitors_with_AR.png", width = 10, height = 6)

write.csv(results_df, "/Users/michelle/Documents/thesis/extended_time_series_with_AR.csv", row.names = FALSE)
```

~ ~ ~ 

## No AR + Quarterly set

```{r}
quarterly_df <- quarterly_df_copy

# Ensure data is quarterly
quarterly_df <- quarterly_df %>%
  mutate(Quarter = as.yearqtr(Quarter)) %>%
  arrange(Quarter)
```

```{r}
# Merge datasets on "Quarter"
quarterly_df <- inflows_quarterly %>%
  left_join(quarterly_df, by = "Quarter")
```

Shorten the covariate series for estimation of the model
```{r}
quarterly_df_2010_16 <- quarterly_df %>%
  filter(!is.na(Kyrgyz_visitors)) %>%
  filter(complete.cases(.))  # Remove rows with NA in any of the predictors
```

```{r}
# Convert time series to ts object
y_full <- quarterly_df_2010_16$Kyrgyz_visitors
```

Covar names
```{r}
# Select potential covariates
covariate_names <- setdiff(names(quarterly_df_2010_16), c("Quarter", "Kyrgyz_visitors"))
```

Scale features according to limited frame and save the scaler for later
```{r}
# Scale the predictor variables (excluding the target variable 'Kyrgyz_visitors')
quarterly_df_2010_16 <- scale(quarterly_df_2010_16[, covariate_names])

# Step 2: Scale the predictor variables (excluding the target variable 'Kyrgyz_visitors')
# Save the scaling parameters (mean and standard deviation) for later use
scaler <- quarterly_df_2010_16
```

Some cause NAs because there's no variation! drop those columns, and save the names for later.
```{r}
# Check for NAs in the dataset
na_count <- colSums(is.na(quarterly_df_2010_16))

# Identify columns with NAs and no variation
cols_to_drop <- names(na_count[na_count > 0])

# Save column names with NAs for later
na_columns <- cols_to_drop

# Optionally, print the names of the columns removed
print(na_columns)

quarterly_df_2010_16 <- as.data.frame(quarterly_df_2010_16)

# Remove the identified columns
quarterly_df_2010_16 <- quarterly_df_2010_16[, !(names(quarterly_df_2010_16) %in% cols_to_drop)]
```

1. Take the top correlated features with the y because otherwise we will have p>n, and we need n>p to do good analysis
```{r}
# Example of using the correlation test for continuous features
cor_values <- apply(quarterly_df_2010_16, 2, function(x) cor(x, y_full))  # Assuming 'y' is the target variable

# Sort the correlations in decreasing order (highest correlation first)
sorted_cor <- sort(cor_values, decreasing = TRUE)

# Select the top 27 features based on least correlation with the target
top_features <- names(sorted_cor)[1:27]

# Subset the data to include only the selected features
selected_data <- quarterly_df_2010_16[, top_features]

# Check the selected features
print(top_features)

quarterly_df_2010_16 <- quarterly_df_2010_16 %>%
  select(top_features)
```

```{r}
# Select potential covariates
covariate_names <- setdiff(names(quarterly_df_2010_16), c("Quarter", "Kyrgyz_visitors"))
```

```{r}
# Load necessary package
library(stats)

# Perform PCA on the scaled predictors
pca_result <- prcomp(quarterly_df_2010_16, center = TRUE, scale. = TRUE)

# Determine how many principal components to keep (e.g., 95% variance explained)
explained_variance <- summary(pca_result)$importance[2,]  # Proportion of variance
cumulative_variance <- cumsum(explained_variance)

# Find the number of components explaining at least 95% variance
num_components <- which(cumulative_variance >= 0.95)[1]

# Keep only the first `num_components` principal components
pca_df <- as.data.frame(pca_result$x[, 1:num_components])

# Add seasonal dummies back (they were not included in PCA)
pca_df <- cbind(pca_df, seasonal_dummies_df[1:28, ])
```

Fit elastic net model
```{r}
library(glmnet)

# Convert response variable to matrix format for LASSO
y <- as.matrix(y_full)

# Convert PCA-transformed predictors to matrix format
X <- as.matrix(pca_df)

# Run LASSO regression
lasso_model <- cv.glmnet(X, y, alpha = .1, standardize = FALSE)

# Get the best lambda value
best_lambda <- lasso_model$lambda.min

# Fit final model using best lambda
final_lasso <- glmnet(X, y, alpha = .1, lambda = best_lambda, standardize = FALSE)

# Extract coefficients
coef(final_lasso)
```

~ ~ ~

Prep to predict
```{r}
quarterly_df <- quarterly_df_copy

# Create seasonal dummies based on the 'Quarter' column
seasonal_dummies <- model.matrix(~ as.factor(quarter(Quarter)) - 1, data = quarterly_df)

# Convert the resulting matrix to a data frame
seasonal_dummies_df <- as.data.frame(seasonal_dummies)

# Set appropriate column names
colnames(seasonal_dummies_df) <- paste0("Q", 1:ncol(seasonal_dummies_df))

quarterly_df <- quarterly_df %>%
  select(-Quarter)

covariate_names <- setdiff(names(quarterly_df), c("Kyrgyz_visitors"))

setdiff(names(quarterly_df), covariate_names)
setdiff(covariate_names, names(quarterly_df))

# Step 5: Scale the predictor variables (excluding the target variable 'Kyrgyz_visitors')
scaler_params <- attr(scaler, "scaled:center")  # Mean
scaler_sd <- attr(scaler, "scaled:scale")      # Standard deviation

# Example of applying the scaler to new data (test data, for example)
quarterly_df <- scale(quarterly_df[, covariate_names], center = scaler_params, scale = scaler_sd)

quarterly_df <- quarterly_df %>%
  as.data.frame()

quarterly_df <- quarterly_df[, !(names(quarterly_df) %in% cols_to_drop)]

# Bind the seasonal dummies to the original data frame (after scaling)
quarterly_df <- cbind(quarterly_df, seasonal_dummies_df)


# Check the scaled data
head(quarterly_df)

quarterly_df_2010_16 <- quarterly_df_2010_16 %>%
  select(top_features)

covariate_names <- setdiff(names(quarterly_df), c("Quarter", "Kyrgyz_visitors"))
```

```{r}
# Apply PCA to the scaled new data (keeping only the first 10 principal components)
new_X_pca <- predict(pca_result, newdata = quarterly_df)[, 1:num_components]

# Ensure seasonal_dummies_df is a data frame and convert it to a matrix if needed
seasonal_dummies_df <- as.data.frame(seasonal_dummies_df)

# Bind the seasonal dummies to the PCA data frame
new_X_pca <- cbind(new_X_pca, seasonal_dummies_df)

# Convert the final data to a matrix before predicting with the lasso model
new_X_pca_matrix <- as.matrix(new_X_pca)

# Step 2: Predict using the lasso model on the transformed data
predictions <- predict(final_lasso, newx = new_X_pca_matrix, s = "lambda.min")

# Print the first few predictions
head(predictions)
```

```{r}
# Step 4: Prepare the results dataframe
combined_actual <- c(inflows_quarterly$Kyrgyz_visitors, rep(NA, length(predictions) - length(inflows_quarterly$Kyrgyz_visitors)))

# Combine y and predictions into a single dataframe for plotting
results_df <- data.frame(
  Quarter = quarterly_df_copy$Quarter,
  Actual = combined_actual,
  Predictions = as.vector(predictions)
)
```

Plot!
```{r}
# Plotting with ggplot
ggplot(results_df, aes(x = Quarter)) +
  geom_line(aes(y = Actual, color = "Actual"), size = 1) +
  geom_line(aes(y = Predictions, color = "Predictions"), size = 1, linetype = "dashed") +
  labs(title = "Actual vs Predicted Kyrgyz Visitors",
       x = "Quarter",
       y = "Kyrgyz Visitors") +
  scale_color_manual(name = "Legend", values = c("Actual" = "blue", "Predictions" = "red")) +
  theme_minimal() +
  geom_vline(xintercept = as.numeric(as.yearqtr("2022 Q1")), linetype = "dashed", color = "black") +  # Vertical line for War starts
  annotate("text", x = as.yearqtr("2022 Q1"), y = max(results_df$Actual, na.rm = TRUE) * 0.9, 
           label = "War begins", angle = 90, hjust = 2, vjust = 0.5, color = "black")
  
ggsave("/Users/michelle/Documents/thesis/plot_extended_time_series_visitors.png", width = 10, height = 6)

write.csv(results_df, "/Users/michelle/Documents/thesis/extended_time_series.csv", row.names = FALSE)
```

